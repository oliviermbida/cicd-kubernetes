# maintainer="Olivier Mbida <oliver.mbida@ai-uavsystems.com>"
version: 2.1

parameters:
  run_build_app:
    type: boolean
    default: true
  run_deploy_infrastructure:
    type: boolean
    default: false  
  run_configure_infrastructure:
    type: boolean
    default: false  
  run_deploy_app:
    type: boolean
    default: true  
  run_update:
    type: boolean
    default: true  
  oldWorkflowID:
    type: string
    default: ""
commands:
# Useful if you don't want to re-run deploy-infrastructure which takes a while.
  check_job:
    description: Stop job if false  
    parameters:
      start_job:
        type: boolean
        default: true        
    steps: 
      - when:
          condition: 
            not: << parameters.start_job >>
          steps:
            - run: circleci-agent step halt
# # Similar to [skip ci] or [ci skip] in commit message
  cancel-workflow:
    description: Check commit message if Circleci build
    steps: 
      - checkout
      - run:
          name: Trigger builds if Circleci
          command: |
            commit_message=$(git log -1 HEAD --pretty=format:%s)
            if [[ $commit_message == *Circleci* ]]; then
              echo "---Circleci CI pipeline---"
            else
              echo "Cancelling workflow ${CIRCLE_WORKFLOW_ID}"
              curl --request POST \
                --url https://circleci.com/api/v2/workflow/${CIRCLE_WORKFLOW_ID}/cancel \
                --header "Circle-Token: ${CIRCLE_TOKEN_API}"     
            fi
# AWS CLI v2           
# Could use the Orb circleci/aws-cli@3.1.1
# Best to know which commands are executed if you are security aware
# You also reduce the overheads of a generic Orb
  install_aws:
    description: Install the AWS CLI via Pip if not already installed.
    parameters:
      binary-dir:
        default: /usr/local/bin
        description: >
          The main aws program in the install directory is symbolically linked to
          the file aws in the specified path. Defaults to /usr/local/bin
        type: string
      install-dir:
        default: /usr/local/aws-cli
        description: >
          Specify the installation directory of AWS CLI. Defaults to
          /usr/local/aws-cli
        type: string
    steps:
      - run:
          command: |
            curl -sSL "https://awscli.amazonaws.com/awscli-exe-linux-x86_64$1.zip" -o "awscliv2.zip"
            unzip -q -o awscliv2.zip
            sudo ./aws/install -i "${PARAM_AWS_CLI_INSTALL_DIR}" -b "${PARAM_AWS_CLI_BINARY_DIR}"
            rm -r awscliv2.zip ./aws
            aws --version
          environment:
            PARAM_AWS_CLI_BINARY_DIR: <<parameters.binary-dir>>
            PARAM_AWS_CLI_INSTALL_DIR: <<parameters.install-dir>>
          name: Install AWS CLI v2
  configure_aws:
    description: >
      configure aws credentials
    parameters:
      access_key_id: 
        type: string
        description: AWS access key Id
        default: $AWS_USER_ACCESS_KEY_ID
      secret_access_key: 
        type: string
        description: AWS secret access key
        default: $AWS_USER_SECRET_ACCESS_KEY
      region: 
        type: string
        description: AWS default region
        default: $AWS_DEFAULT_REGION
    steps: 
      - run:
          name: Configure aws 
          command: |
            # AWS CLI supported environment variables
            AWS_ACCESS_KEY_ID=$(eval echo "$PARAM_AWS_CLI_ACCESS_KEY_ID")
            AWS_SECRET_ACCESS_KEY=$(eval echo "$PARAM_AWS_CLI_SECRET_ACCESS_KEY")
            AWS_DEFAULT_REGION=$(eval echo "$PARAM_AWS_CLI_REGION")
            # configure aws for this job
            aws configure set aws_access_key_id "$AWS_ACCESS_KEY_ID" 
            aws configure set aws_secret_access_key "$AWS_SECRET_ACCESS_KEY" 
            # cleanup
            unset AWS_ACCESS_KEY_ID
            unset AWS_USER_SECRET_ACCESS_KEY
          environment:
            PARAM_AWS_CLI_ACCESS_KEY_ID: <<parameters.access_key_id>>
            PARAM_AWS_CLI_REGION: <<parameters.region>>
            PARAM_AWS_CLI_SECRET_ACCESS_KEY: <<parameters.secret_access_key>>

# 
jobs:
  # https://circleci.com/docs/building-docker-images          
  build-app:
    # docker:
    #   - image: cimg/base:2020.05  
    # # machine: true
    # resource_class: $CIRCLECI_RESOURCE_CLASS  
    # # docker:
    # #   - image: cimg/base:2020.05 
    machine:
      image: ubuntu-2204:2022.04.2    
    steps:
      # - setup_remote_docker:
      #     version: 20.10.14  
      - check_job:
          start_job: <<pipeline.parameters.run_build_app>>
      # - install_aws
      - configure_aws:
          access_key_id: $AWS_USER_ACCESS_KEY_ID
          secret_access_key: $AWS_USER_SECRET_ACCESS_KEY
      - checkout
      - run:
          name: install dependencies
          command: |
            sudo apt-get update 
            sudo apt install -y jq
            sudo apt-get install -y docker-compose-plugin
            npm install snyk -g
      - run:
          name: install linter
          command: |
            make setup
            make install          
      - run:
          name: run lint
          command: |
            . ~/.devops/bin/activate
            make lint    
      - run:
          name: Build app
          command: |
            # Get dockerhub credentials from aws secretsmanager
            DOCKERHUB_USERNAME=$(aws ssm get-parameter \
                --name /aws/reference/secretsmanager/dockerhub \
                --with-decryption --output text --query "Parameter.Value" | jq -r ."username")
            DOCKERHUB_PASSWORD=$(aws ssm get-parameter \
                --name /aws/reference/secretsmanager/dockerhub \
                --with-decryption --output text --query "Parameter.Value" | jq -r ."password") 
            export WORKFLOW_ID=${CIRCLE_WORKFLOW_ID:0:7} 
            printf "%s/%s:%s" "${DOCKERHUB_USERNAME}" "notebook" "${WORKFLOW_ID}" > /tmp/docker_tag.out
            dockerpath=$(cat /tmp/docker_tag.out)
            echo "Docker ID and Image: $dockerpath"
            docker build --tag $dockerpath .
            docker image ls
      - run:
          name: Scan app
          command: |
            # Get dockerhub credentials from aws secretsmanager
            DOCKERHUB_USERNAME=$(aws ssm get-parameter \
                --name /aws/reference/secretsmanager/dockerhub \
                --with-decryption --output text --query "Parameter.Value" | jq -r ."username")
            DOCKERHUB_PASSWORD=$(aws ssm get-parameter \
                --name /aws/reference/secretsmanager/dockerhub \
                --with-decryption --output text --query "Parameter.Value" | jq -r ."password") 
            echo "${DOCKERHUB_PASSWORD}" | docker login -u "${DOCKERHUB_USERNAME}" --password-stdin 
            dockerpath=$(cat /tmp/docker_tag.out)
            # "true" to fix Bug exit code 141 even though scan passed.
            # remove to get updated scan
            yes Y | docker scan $dockerpath || true
      - run:
          name: Test app
          command: |
            dockerpath=$(cat /tmp/docker_tag.out)
            export dockerpath
            docker compose run notebook npm test              
      - run:
          name: Push app to dockerhub
          command: |
            # Get dockerhub credentials from aws secretsmanager
            DOCKERHUB_USERNAME=$(aws ssm get-parameter \
                --name /aws/reference/secretsmanager/dockerhub \
                --with-decryption --output text --query "Parameter.Value" | jq -r ."username")
            DOCKERHUB_PASSWORD=$(aws ssm get-parameter \
                --name /aws/reference/secretsmanager/dockerhub \
                --with-decryption --output text --query "Parameter.Value" | jq -r ."password") 
            export dockerpath=$(cat /tmp/docker_tag.out)
            echo "Docker ID and Image: $dockerpath"
            echo "${DOCKERHUB_PASSWORD}" | docker login -u "${DOCKERHUB_USERNAME}" --password-stdin 
            docker push "$dockerpath"             
      - run:
          name: Cleanup docker config
          when: on_fail
          command: |
            # WARNING! Your password will be stored unencrypted in ~/.docker/config.json.
            # Rotate dockerhub and AWS ECR credentials
            # To reduce security risk configure docker credential store before login
            # https://docs.docker.com/engine/reference/commandline/login/#credentials-store
            if [ -f ~/.docker/config.json ]; then
              rm -rf ~/.docker/config.json
            fi
      - run:
          name: Cleanup aws config
          when: on_fail
          command: |
            # cleanup
            rm -f $AWS_SHARED_CREDENTIALS_FILE
            rm -f $AWS_CONFIG_FILE
#
  deploy-infrastructure:
    docker:
      - image: cimg/base:2020.05
    steps:
      - check_job:
          start_job: <<pipeline.parameters.run_deploy_infrastructure>>
      - checkout
      - install_aws
      - configure_aws:
          access_key_id: $AWS_ADMIN_ACCESS_KEY_ID
          secret_access_key: $AWS_ADMIN_SECRET_ACCESS_KEY        
      - run:
          name: Install dependencies
          command: |
            # curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
            # curl -LO "https://dl.k8s.io/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl.sha256"
            # echo "$(cat kubectl.sha256)  kubectl" | sha256sum --check >/dev/null 2>&1 || \
            curl -o kubectl https://s3.us-west-2.amazonaws.com/amazon-eks/1.23.7/2022-06-29/bin/linux/amd64/kubectl
            sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl
            sudo apt update
            sudo apt install -y python3-pip
            pip3 install envsubst
      - run:
          name: Deploy kubernetes cluster 
          no_output_timeout: 30m
          command: |
            export STACKNAME=$CLUSTERNAME 
            export BUCKET="eks-tmp-$(LC_CTYPE=C tr -dc 'a-z0-9' </dev/urandom | fold -w 16 | head -n 1)"
            aws s3 mb s3://${BUCKET} --region ${AWS_REGION} 
            envsubst < ./templates/policy.json | tee ./templates/_policy.json > /dev/null
            aws s3api put-bucket-policy --bucket ${BUCKET} --policy file://./templates/_policy.json --region ${AWS_REGION} 
            aws cloudformation package --template-file ./templates/cluster.yaml \
              --s3-bucket ${BUCKET} \
              --output-template-file ./templates/cluster-packaged.yaml
            while [ ! -f "./templates/cluster-packaged.yaml" ] ; do 
              echo "..."
            done 
            aws cloudformation deploy --template-file ./templates/cluster-packaged.yaml \
             --stack-name ${STACKNAME} --region ${AWS_REGION} --capabilities CAPABILITY_NAMED_IAM
      - run:
          name: Configure kubectl
          command: |
            aws eks update-kubeconfig --name $CLUSTERNAME
            # aws eks describe-cluster --name $CLUSTERNAME
      - run:
          name: Test KubeConfig
          command: |
            # kubectl config show
            kubectl version --short
            kubectl get namespaces --show-labels
      - run:
          name: Cleanup 
          command: |
            # cleanup
            rm -rf ~/.kube 
            rm -f $AWS_SHARED_CREDENTIALS_FILE
            rm -f $AWS_CONFIG_FILE            
      - run:
          name: Cleanup 
          when: on_fail
          command: |
            # cleanup
            if [ -d ~/.kube ]; then 
              rm -rf ~/.kube  
            fi
            rm -f $AWS_SHARED_CREDENTIALS_FILE
            rm -f $AWS_CONFIG_FILE  
#            
  configure-infrastructure:
    docker:
      - image: cimg/base:2020.05
    steps:
      - check_job:
          start_job: <<pipeline.parameters.run_configure_infrastructure>>
      - checkout
      - install_aws      
      - configure_aws:
          access_key_id: $AWS_ADMIN_ACCESS_KEY_ID
          secret_access_key: $AWS_ADMIN_SECRET_ACCESS_KEY
      - run:
          name: Install dependencies
          command: |
            # curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
            # curl -LO "https://dl.k8s.io/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl.sha256"
            # echo "$(cat kubectl.sha256)  kubectl" | sha256sum --check >/dev/null 2>&1 || \
            curl -o kubectl https://s3.us-west-2.amazonaws.com/amazon-eks/1.23.7/2022-06-29/bin/linux/amd64/kubectl
            sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl
            #kubectl version --short
            curl --silent --location "https://github.com/weaveworks/eksctl/releases/latest/download/eksctl_$(uname -s)_amd64.tar.gz" \
             | tar xz -C /tmp
            sudo mv /tmp/eksctl /usr/local/bin
            # eksctl version
            curl https://baltocdn.com/helm/signing.asc | \
            gpg --dearmor | sudo tee /usr/share/keyrings/helm.gpg > /dev/null
            sudo apt-get install apt-transport-https --yes
            echo "deb [arch=$(dpkg --print-architecture) \
            signed-by=/usr/share/keyrings/helm.gpg] \
            https://baltocdn.com/helm/stable/debian/ all main" | \
            sudo tee /etc/apt/sources.list.d/helm-stable-debian.list
            sudo apt-get update
            sudo apt-get install helm
            helm version
            sudo apt update
            sudo apt install -y python3-pip
            pip3 install envsubst            
      - run:
          name: Configure kubectl
          command: |
            aws eks update-kubeconfig --name ${CLUSTERNAME} 
            # aws eks describe-cluster --name ${CLUSTERNAME} 
            kubectl version --short
            eksctl version
      - run:
          name: Configure OIDC provider & IAM role 
          command: |
            oidc_id=$(aws eks describe-cluster --name ${CLUSTERNAME} --query "cluster.identity.oidc.issuer" --output text | cut -d '/' -f 5)
            if [ $oidc_id != "" ]; then 
            	eksctl utils associate-iam-oidc-provider --cluster $CLUSTERNAME --approve
            else
              exit 1
            fi
            eksctl create iamserviceaccount --cluster=$CLUSTERNAME --namespace=kube-system --name=aws-load-balancer-controller \
              --role-name "AmazonEKSLoadBalancerControllerRole" \
              --attach-policy-arn=arn:aws:iam::${AWS_ACCOUNT_ID}:policy/AWSLoadBalancerControllerIAMPolicy --approve            
      - run:
          name: Install AWS Load Balancer Controller add-on
          command: |
            kubectl apply \
            --validate=false \
            -f https://github.com/jetstack/cert-manager/releases/download/v1.5.4/cert-manager.yaml
            curl -Lo v2_4_3_full.yaml https://github.com/kubernetes-sigs/aws-load-balancer-controller/releases/download/v2.4.3/v2_4_3_full.yaml
            while [ ! -f "./v2_4_3_full.yaml" ] ; do 
              echo "..."
            done 
            sed -i.bak -e '480,488d' ./v2_4_3_full.yaml
            sed -i.bak -e 's|your-cluster-name|${CLUSTERNAME}|' ./v2_4_3_full.yaml
            kubectl apply -f v2_4_3_full.yaml
            kubectl rollout status -f v2_4_3_full.yaml --timeout=15s	   
            curl -Lo v2_4_3_ingclass.yaml https://github.com/kubernetes-sigs/aws-load-balancer-controller/releases/download/v2.4.3/v2_4_3_ingclass.yaml
            while [ ! -f "./v2_4_3_ingclass.yaml" ] ; do 
              echo "..."
            done             
            kubectl apply -f v2_4_3_ingclass.yaml
            kubectl get deployment -n kube-system aws-load-balancer-controller
      - run:
          name: Rollback Load balancer
          when: on_fail
          command: |
            kubectl delete deployment -n kube-system aws-load-balancer-controller || true
            kubectl delete -f v2_4_3_ingclass.yaml || true
#  
  deploy-app:
    docker:
      - image: cimg/base:2020.05
    steps:
      - check_job:
          start_job: <<pipeline.parameters.run_deploy_app>>
      - checkout
      - install_aws      
      - configure_aws:
          access_key_id: $AWS_ADMIN_ACCESS_KEY_ID
          secret_access_key: $AWS_ADMIN_SECRET_ACCESS_KEY
      - run:
          name: Install dependencies
          command: |
            # curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
            # curl -LO "https://dl.k8s.io/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl.sha256"
            # echo "$(cat kubectl.sha256)  kubectl" | sha256sum --check >/dev/null 2>&1 || \
            curl -o kubectl https://s3.us-west-2.amazonaws.com/amazon-eks/1.23.7/2022-06-29/bin/linux/amd64/kubectl
            sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl
            curl --silent --location "https://github.com/weaveworks/eksctl/releases/latest/download/eksctl_$(uname -s)_amd64.tar.gz" \
             | tar xz -C /tmp
            sudo mv /tmp/eksctl /usr/local/bin
            eksctl version
            curl https://baltocdn.com/helm/signing.asc | gpg --dearmor | sudo tee /usr/share/keyrings/helm.gpg > /dev/null
            sudo apt-get install apt-transport-https --yes
            echo "deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/helm.gpg] https://baltocdn.com/helm/stable/debian/ all main" | sudo tee /etc/apt/sources.list.d/helm-stable-debian.list
            sudo apt-get update
            sudo apt-get install helm
            helm version
            sudo apt install -y python3-pip
            pip3 install envsubst            
      - run:
          name: Configure kubectl
          command: |
            aws eks update-kubeconfig --name ${CLUSTERNAME}
            kubectl version --short
            # aws eks describe-cluster --name ${CLUSTERNAME} 	
      - run: 
          name: Configure dockerhub secrets
          command: |
            DOCKERHUB_USERNAME=$(aws ssm get-parameter \
            --name /aws/reference/secretsmanager/dockerhub \
            --with-decryption --output text --query "Parameter.Value" | jq -r ."username")
            DOCKERHUB_PASSWORD=$(aws ssm get-parameter \
            --name /aws/reference/secretsmanager/dockerhub \
            --with-decryption --output text --query "Parameter.Value" | jq -r ."password") 
            kubectl describe secrets/dockerhub || \
            kubectl create secret docker-registry dockerhub \
            --docker-server=docker.io \
            --docker-username=$DOCKERHUB_USERNAME \
            --docker-password=$DOCKERHUB_PASSWORD								
      - run:
          name: Get old workflow ID
          command: |
            OLD_BUILD_NUM=$( echo $(( CIRCLE_BUILD_NUM - 1 )) ) 
            artifacts=$(curl -X GET "https://circleci.com/api/v2/project/github/oliviermbida/cicd-kubernetes/$OLD_BUILD_NUM/artifacts" \
            -H "Accept: application/json" \
            -u "$CIRCLE_TOKEN_API:") || true
            echo "read -f '' OLD_WORKFLOW_ID \<< 'EOF_ARTIFACTS'" >> $BASH_ENV
            echo "$artifacts" >> $BASH_ENV
            echo "EOF_ARTIFACTS" >> $BASH_ENV
            echo $OLD_WORKFLOW_ID
      - run:
          name: Deploy App environment to cluster
          command: |
            # Set environment variables
            export DB_USER=$(aws ssm get-parameter \
                --name /aws/reference/secretsmanager/UdaPeopleDB \
                --with-decryption --output text --query "Parameter.Value" | jq -r ."username")
            export DB_PASSWORD=$(aws ssm get-parameter \
                --name /aws/reference/secretsmanager/UdaPeopleDB \
                --with-decryption --output text --query "Parameter.Value" | jq -r ."password")
            export DB_SCHEMA="postgres"
            export DB_PORT=$(aws rds describe-db-instances \
                --db-instance-identifier backendDB \
                --output text --query "DBInstances[*].Endpoint.Port")
            export DB_HOST=$(aws rds describe-db-instances \
                --db-instance-identifier backendDB \
                --output text --query "DBInstances[*].Endpoint.Address")              
            export DOCKERHUB_USERNAME=$(aws ssm get-parameter \
            --name /aws/reference/secretsmanager/dockerhub \
            --with-decryption --output text --query "Parameter.Value" | jq -r ."username")      		
            export WORKFLOW_ID=${CIRCLE_WORKFLOW_ID:0:7} 
            #
            # Setup production environment
            kubectl create namespace "notebook-${WORKFLOW_ID}"
            kubectl get namespaces --show-labels   
            #
            # Deploy latest version of App
            envsubst < manifests/deployment.yml | tee deployment.yml > /dev/null
            kubectl apply --namespace notebook-${WORKFLOW_ID} -f ./deployment.yml
            # kubectl rollout status -f deployment.yml --timeout=10s | grep -oh 'deployment "notebook-Â£{WORKFLOW_ID}" successfully rolled out' || echo exit 1 
            kubectl get pods -l app=notebook-${WORKFLOW_ID} --namespace notebook-${WORKFLOW_ID} 
            envsubst < manifests/service.yml | tee service.yml > /dev/null
            kubectl apply -f ./service.yml --namespace notebook-${WORKFLOW_ID} 
            kubectl describe service notebook-${WORKFLOW_ID} --namespace notebook-${WORKFLOW_ID}
      - run:
          name: Ensure prod environment exist
          command: |
            export WORKFLOW_ID=${CIRCLE_WORKFLOW_ID:0:7} 
            # kubectl exec -it $(kubectl get pod -l app=notebook-${WORKFLOW_ID} -o name | head -n 1) -- npm run test || true	
            # kubectl exec $(kubectl get pod -l app=notebook-${WORKFLOW_ID} -o name --namespace notebook-${WORKFLOW_ID} | head -n 1) --namespace notebook-${WORKFLOW_ID} -- env
      - run:
          name: Rollback deployment 
          command: |
            export WORKFLOW_ID=${CIRCLE_WORKFLOW_ID:0:7} 
            echo "Notebook environment deployment failed"
            #kubectl delete namespace notebook-${WORKFLOW_ID} || true
            #kubectl delete deployment/notebook-${WORKFLOW_ID} || true
            #kubectl delete svc notebook-${WORKFLOW_ID} || true
          condition: on_fail
          
  update:
    docker:
      - image: cimg/base:2020.05
    steps:
      - check_job:
          start_job: <<pipeline.parameters.run_update>>
      - checkout
      - install_aws      
      - configure_aws:
          access_key_id: $AWS_ADMIN_ACCESS_KEY_ID
          secret_access_key: $AWS_ADMIN_SECRET_ACCESS_KEY
      - run:
          name: Install dependencies
          command: |
            # curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
            # curl -LO "https://dl.k8s.io/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl.sha256"
            # echo "$(cat kubectl.sha256)  kubectl" | sha256sum --check >/dev/null 2>&1 || \
            curl -o kubectl https://s3.us-west-2.amazonaws.com/amazon-eks/1.23.7/2022-06-29/bin/linux/amd64/kubectl
            sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl
            curl --silent --location "https://github.com/weaveworks/eksctl/releases/latest/download/eksctl_$(uname -s)_amd64.tar.gz" \
             | tar xz -C /tmp
            sudo mv /tmp/eksctl /usr/local/bin
            eksctl version
            curl https://baltocdn.com/helm/signing.asc | gpg --dearmor | sudo tee /usr/share/keyrings/helm.gpg > /dev/null
            sudo apt-get install apt-transport-https --yes
            echo "deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/helm.gpg] https://baltocdn.com/helm/stable/debian/ all main" | sudo tee /etc/apt/sources.list.d/helm-stable-debian.list
            sudo apt-get update
            sudo apt-get install helm
            helm version
            sudo apt install -y python3-pip
            pip3 install envsubst              
      - run:
          name: Configure kubectl
          command: |
            aws eks update-kubeconfig --name ${CLUSTERNAME}
            # aws eks describe-cluster --name ${CLUSTERNAME}
            kubectl version --short
      - run:
          name: Promotion notebook
          command: |
            export WORKFLOW_ID=${CIRCLE_WORKFLOW_ID:0:7} 
            export OLD_WORKFLOW_ID=<<pipeline.parameters.oldWorkflowID>>
            # Deploy latest version of App
            envsubst < manifests/ingress.yml | tee ingress.yml > /dev/null					
            kubectl apply -f ./ingress.yml	--namespace notebook-${WORKFLOW_ID}
            kubectl get ingress/notebook-${WORKFLOW_ID} --namespace notebook-${WORKFLOW_ID}
            if [ ${OLD_WORKFLOW_ID} != "" ]; then
              kubectl delete ingress/notebook-${OLD_WORKFLOW_ID} --namespace notebook-${OLD_WORKFLOW_ID}
            fi
      - run:
          name: Smoke Test notebook
          command: |
            export WORKFLOW_ID=${CIRCLE_WORKFLOW_ID:0:7}
            ALB_URL=$(kubectl get ingress/notebook-${WORKFLOW_ID} -n notebook-${WORKFLOW_ID} -o json | jq ".status.loadBalancer.ingress[0].hostname")
            curl -S $URL | grep -oh "Welcome to Express" > /tmp/FRONTEND_SMOKE_TEST
            echo "Frontend smoke test"
            GREEN='\033[0;32m'       
            RED='\033[0;31m'        
            NC='\033[0m' 
            TEST_PASSED=$(cat /tmp/FRONTEND_SMOKE_TEST)
            if [[ $TEST_PASSED == "Welcome to Express" ]]; then
              printf "TEST: ${GREEN}PASSED${NC}\n"
            else
              printf "TEST: ${RED}FAILED${NC}\n"
              # trigger rollback
              exit 1
            fi
      - run:
          name: store artifacts
          command: |
            # Store workflow ID for next build
            mkdir -p /tmp/artifacts
            echo "${WORKFLOW_ID}" > /tmp/artifacts/OLD_WORKFLOW_ID          
      - store_artifacts:
          path: /tmp/artifacts            
      - run:
          name: Rollback promotion
          command: |
            # swap workflow ids to revert ingress.yml
            export OLD_WORKFLOW_ID=${<<pipeline.parameters.oldWorkflowID>>}
            export WORKFLOW_ID=${CIRCLE_WORKFLOW_ID:0:7}
            echo "Workflow ID ${WORKFLOW_ID} environment deployment failed"
            if [ ${OLD_WORKFLOW_ID} != "" ]; then
              envsubst < manifests/ingress-old.yml | tee ingress-old.yml > /dev/null					
              kubectl apply -f ./ingress-old.yml	--namespace notebook-${OLD_WORKFLOW_ID}
              kubectl get ingress/notebook-${OLD_WORKFLOW_ID} --namespace notebook-${OLD_WORKFLOW_ID}
            fi
            kubectl delete --namespace notebook-${WORKFLOW_ID}
          condition: on_fail           
# build-app job is run on tags since the docker image pulled in production is tagged with same Workflow ID             
workflows:
  default:
    jobs:
      - build-app:
          pre-steps:
            - cancel-workflow      
          context:
            - org-global
            - aws-context
          filters:
            branches:
              only: master
            tags:
              only: /^v.*/              
      - deploy-infrastructure:
          requires:
            - build-app      
          context:
            - org-global
            - aws-context
          filters: &filters-production
            branches:
              only: master
            tags:
              only: /^v.*/      
      - configure-infrastructure:
          requires: 
            - deploy-infrastructure              
          context:
            - org-global
            - aws-context
          filters:
            <<: *filters-production
      - deploy-app:
          requires: 
            - configure-infrastructure              
          context:
            - org-global
            - aws-context
          filters:
            <<: *filters-production    
      - approve-update:
          type: approval
          requires: 
            - deploy-app       
      - update:
          requires: 
            - approve-update             
          context:
            - org-global
            - aws-context
          filters:
            <<: *filters-production              